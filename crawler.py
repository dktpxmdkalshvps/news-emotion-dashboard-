"""
crawler.py - 3ëŒ€ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í†µí•© í¬ë¡¤ëŸ¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ê³µí†µ ì¸í„°íŽ˜ì´ìŠ¤(BaseCrawler) â†’ ì‚¬ì´íŠ¸ë³„ êµ¬í˜„ì²´ â†’ í†µí•© ë§¤ë‹ˆì € êµ¬ì¡°

  BaseCrawler (ì¶”ìƒ í´ëž˜ìŠ¤)
    â”œâ”€â”€ NaverCrawler     ë„¤ì´ë²„ ë‰´ìŠ¤
    â”œâ”€â”€ DaumCrawler      ë‹¤ìŒ ë‰´ìŠ¤
    â””â”€â”€ HankyungCrawler  í•œêµ­ê²½ì œ

  MultiSiteCrawler      ì„¸ í¬ë¡¤ëŸ¬ë¥¼ ë¬¶ì–´ í•œ ë²ˆì— ì‹¤í–‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

from __future__ import annotations

import time
import random
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from urllib.parse import quote_plus

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  ë°ì´í„° í´ëž˜ìŠ¤
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
@dataclass
class NewsItem:
    title:      str
    press:      str
    pub_time:   str
    url:        str
    source:     str
    crawled_at: str = field(
        default_factory=lambda: datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )

    def to_dict(self) -> dict:
        return {
            "title":      self.title,
            "press":      self.press,
            "pub_time":   self.pub_time,
            "url":        self.url,
            "source":     self.source,
            "crawled_at": self.crawled_at,
        }


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  ê³µí†µ ë“œë¼ì´ë²„ íŒ©í† ë¦¬
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
def build_driver(headless: bool = True) -> webdriver.Chrome:
    options = Options()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    )
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    driver = webdriver.Chrome(options=options)
    driver.execute_script(
        "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    )
    return driver


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  ê³µí†µ ì¸í„°íŽ˜ì´ìŠ¤ (ì¶”ìƒ ë² ì´ìŠ¤ í´ëž˜ìŠ¤)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class BaseCrawler(ABC):
    """
    ëª¨ë“  ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ëŸ¬ê°€ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ê³µí†µ ì¸í„°íŽ˜ì´ìŠ¤.

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  êµ¬í˜„ ì˜ë¬´ ë©”ì„œë“œ (ì¶”ìƒ)                              â”‚
    â”‚  â”œâ”€â”€ build_url(keyword, page)  â†’ ê²€ìƒ‰ URL ìƒì„±       â”‚
    â”‚  â”œâ”€â”€ wait_selector()           â†’ ë¡œë”© ëŒ€ê¸° CSS ì…€ë ‰í„°â”‚
    â”‚  â””â”€â”€ parse_page(driver)        â†’ í•­ëª© íŒŒì‹± ë¡œì§      â”‚
    â”‚                                                      â”‚
    â”‚  ê³µí†µ ì œê³µ ë©”ì„œë“œ (ìž¬ì‚¬ìš©)                            â”‚
    â”‚  â””â”€â”€ crawl(keyword, pages)     â†’ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """

    SITE_NAME: str = ""
    SITE_KEY:  str = ""

    def __init__(self, headless: bool = True, wait_sec: int = 8,
                 delay_range: tuple = (1.2, 2.5)):
        self.headless    = headless
        self.wait_sec    = wait_sec
        self.delay_range = delay_range
        self._driver = None

    # â”€â”€ ì¶”ìƒ ë©”ì„œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @abstractmethod
    def build_url(self, keyword: str, page: int) -> str:
        """í‚¤ì›Œë“œ + íŽ˜ì´ì§€ ë²ˆí˜¸ë¡œ ê²€ìƒ‰ URLì„ ìƒì„±í•©ë‹ˆë‹¤."""

    @abstractmethod
    def wait_selector(self) -> str:
        """íŽ˜ì´ì§€ ë¡œë”© ì™„ë£Œë¥¼ íŒë‹¨í•  CSS ì…€ë ‰í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""

    @abstractmethod
    def parse_page(self, driver: webdriver.Chrome) -> list:
        """í˜„ìž¬ íŽ˜ì´ì§€ì—ì„œ NewsItem ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""

    # â”€â”€ ê³µí†µ ì‹¤í–‰ ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def crawl(self, keyword: str, pages: int = 5) -> list:
        """ì§€ì • í‚¤ì›Œë“œë¡œ pages ìˆ˜ë§Œí¼ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        results = []
        self._driver = build_driver(self.headless)
        wait = WebDriverWait(self._driver, self.wait_sec)

        try:
            for page in range(1, pages + 1):
                url = self.build_url(keyword, page)
                print(f"      [{self.SITE_NAME}] {page}/{pages}p â†’ {url}")
                self._driver.get(url)

                try:
                    wait.until(EC.presence_of_element_located(
                        (By.CSS_SELECTOR, self.wait_selector())
                    ))
                except TimeoutException:
                    print(f"      âš ï¸  íƒ€ìž„ì•„ì›ƒ - {page}p ê±´ë„ˆëœ€")
                    continue

                try:
                    items = self.parse_page(self._driver)
                except Exception as e:
                    print(f"      âš ï¸  íŒŒì‹± ì˜¤ë¥˜: {e}")
                    items = []

                results.extend(items)
                print(f"      âœ“ {len(items)}ê±´ ìˆ˜ì§‘ (ëˆ„ì  {len(results)}ê±´)")
                time.sleep(random.uniform(*self.delay_range))

        except Exception as e:
            print(f"      âŒ [{self.SITE_NAME}] í¬ë¡¤ë§ ì¤‘ë‹¨: {e}")
        finally:
            if self._driver:
                self._driver.quit()
                self._driver = None

        return results

    # â”€â”€ ê³µí†µ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def safe_text(element, selector: str, default: str = "") -> str:
        """CSS ì…€ë ‰í„°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            return element.find_element(By.CSS_SELECTOR, selector).text.strip()
        except NoSuchElementException:
            return default

    @staticmethod
    def safe_attr(element, selector: str, attr: str, default: str = "") -> str:
        """CSS ì…€ë ‰í„°ë¡œ ì†ì„±ê°’ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            return element.find_element(By.CSS_SELECTOR, selector).get_attribute(attr) or default
        except NoSuchElementException:
            return default


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  êµ¬í˜„ì²´ 1 â€” ë„¤ì´ë²„ ë‰´ìŠ¤
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class NaverCrawler(BaseCrawler):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í¬ë¡¤ëŸ¬

    URL íŒ¨í„´:
      search.naver.com/search.naver?where=news&query={kw}&start={start}&sort=1
      â””â”€â”€ start: 1íŽ˜ì´ì§€=1, 2íŽ˜ì´ì§€=11, 3íŽ˜ì´ì§€=21 ... (Ã—10 ì˜¤í”„ì…‹)

    ìˆ˜ì§‘ ì…€ë ‰í„°:
      ëª©ë¡  ul.list_news > li.bx
      ì œëª©  a.news_tit
      ì–¸ë¡ ì‚¬ a.info.press
      ì‹œê°„  span.info (ë‚ ì§œ íŒ¨í„´ í¬í•¨ ìš”ì†Œ)
    """
    SITE_NAME = "ë„¤ì´ë²„"
    SITE_KEY  = "naver"

    _BASE = ("https://search.naver.com/search.naver"
             "?where=news&query={kw}&start={start}&sort=1")

    def build_url(self, keyword: str, page: int) -> str:
        start = (page - 1) * 10 + 1
        return self._BASE.format(kw=quote_plus(keyword), start=start)

    def wait_selector(self) -> str:
        return "ul.list_news > li"

    def parse_page(self, driver) -> list:
        items = []
        cards = driver.find_elements(By.CSS_SELECTOR, "ul.list_news > li.bx")
        for card in cards:
            title = self.safe_text(card, "a.news_tit")
            if not title:
                continue
            press    = self.safe_text(card, "a.info.press") or self.safe_text(card, "a.press")
            pub_time = self._extract_time(card)
            url      = self.safe_attr(card, "a.news_tit", "href")
            items.append(NewsItem(title=title, press=press or "ì•Œ ìˆ˜ ì—†ìŒ",
                                  pub_time=pub_time, url=url, source=self.SITE_KEY))
        return items

    @staticmethod
    def _extract_time(card) -> str:
        spans = card.find_elements(By.CSS_SELECTOR, "span.info")
        for span in spans:
            text = span.text.strip()
            if any(k in text for k in ["ì „", ".", "ì‹œê°„", "ì¼"]):
                return text
        return spans[-1].text.strip() if spans else ""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  êµ¬í˜„ì²´ 2 â€” ë‹¤ìŒ ë‰´ìŠ¤
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class DaumCrawler(BaseCrawler):
    """
    ë‹¤ìŒ(Daum) ë‰´ìŠ¤ ê²€ìƒ‰ í¬ë¡¤ëŸ¬

    URL íŒ¨í„´:
      search.daum.net/search?w=news&q={kw}&p={page}&sort=recency
      â””â”€â”€ p: 1íŽ˜ì´ì§€=1, 2íŽ˜ì´ì§€=2 ... (ê·¸ëŒ€ë¡œ íŽ˜ì´ì§€ ë²ˆí˜¸)

    ìˆ˜ì§‘ ì…€ë ‰í„°:
      ëª©ë¡  li.g_item  /  div.cont_inner
      ì œëª©  a.tit_main  /  a.link_txt  /  a.item-title
      ì–¸ë¡ ì‚¬ span.name_cp  /  span.txt_cp
      ì‹œê°„  span.num_date  /  span.date_txt
    """
    SITE_NAME = "ë‹¤ìŒ"
    SITE_KEY  = "daum"

    _BASE = ("https://search.daum.net/search"
             "?w=news&q={kw}&p={page}&spacing=0&sort=recency")

    def build_url(self, keyword: str, page: int) -> str:
        return self._BASE.format(kw=quote_plus(keyword), page=page)

    def wait_selector(self) -> str:
        return "div#newsSearchMainList, ul.list_news, div.wrap_g"

    def parse_page(self, driver) -> list:
        items = []
        cards = []
        for sel in ["li.g_item", "div.cont_inner", "li[data-docid]"]:
            cards = driver.find_elements(By.CSS_SELECTOR, sel)
            if cards:
                break

        for card in cards:
            title = (self.safe_text(card, "a.tit_main")
                     or self.safe_text(card, "a.link_txt")
                     or self.safe_text(card, "a.item-title")
                     or self.safe_text(card, "a.tit_g"))
            if not title:
                continue

            url = (self.safe_attr(card, "a.tit_main", "href")
                   or self.safe_attr(card, "a.link_txt", "href")
                   or self.safe_attr(card, "a.item-title", "href"))
            press = (self.safe_text(card, "span.name_cp")
                     or self.safe_text(card, "span.txt_cp")
                     or self.safe_text(card, "span.info_txt"))
            pub_time = (self.safe_text(card, "span.num_date")
                        or self.safe_text(card, "span.date_txt")
                        or self.safe_text(card, "span.info_date"))
            items.append(NewsItem(title=title, press=press or "ì•Œ ìˆ˜ ì—†ìŒ",
                                  pub_time=pub_time, url=url, source=self.SITE_KEY))
        return items


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  êµ¬í˜„ì²´ 3 â€” í•œêµ­ê²½ì œ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class HankyungCrawler(BaseCrawler):
    """
    í•œêµ­ê²½ì œ(hankyung.com) ë‰´ìŠ¤ ê²€ìƒ‰ í¬ë¡¤ëŸ¬

    URL íŒ¨í„´:
      www.hankyung.com/search?search_str={kw}&page={page}&type=news&sort=date
      â””â”€â”€ page: 1íŽ˜ì´ì§€=1, 2íŽ˜ì´ì§€=2 ...
    ìˆ˜ì§‘ ì…€ë ‰í„°:
      ëª©ë¡  li.item  /  article.list-item
      ì œëª©  .news-tit  /  h3.title a  /  a.tit
      ì–¸ë¡ ì‚¬ span.author  (ìžì‚¬ ê¸°ì‚¬ ë§ŽìŒ â†’ ê¸°ìžëª… ëŒ€ì²´)
      ì‹œê°„  span.date  /  time

    íŠ¹ì§•:
      ê²½ì œ/ì‚°ì—… ì „ë¬¸ ìš©ì–´ ë‹¤ìˆ˜ â†’ ê°ì„± ì‚¬ì „ ê°€ì¤‘ì¹˜ ë¶„ì„ì— ìœ ë¦¬
      ìƒëŒ€ ê²½ë¡œ URL â†’ https://www.hankyung.com ìžë™ prefix ì²˜ë¦¬
    """
    SITE_NAME = "í•œêµ­ê²½ì œ"
    SITE_KEY  = "hankyung"

    _BASE = ("https://www.hankyung.com/search"
             "?search_str={kw}&page={page}&type=news&sort=date")

    def build_url(self, keyword: str, page: int) -> str:
        return self._BASE.format(kw=quote_plus(keyword), page=page)

    def wait_selector(self) -> str:
        return "ul.list-news, div.news-list, article.news-item"

    def parse_page(self, driver) -> list:
        items = []
        cards = []
        for sel in ["li.item", "li.news-item", "article.list-item"]:
            cards = driver.find_elements(By.CSS_SELECTOR, sel)
            if cards:
                break

        for card in cards:
            title = (self.safe_text(card, ".news-tit")
                     or self.safe_text(card, "h3.title a")
                     or self.safe_text(card, "a.tit")
                     or self.safe_text(card, ".tit"))
            if not title:
                continue

            url = (self.safe_attr(card, ".news-tit", "href")
                   or self.safe_attr(card, "h3.title a", "href")
                   or self.safe_attr(card, "a.tit", "href"))
            if url and url.startswith("/"):
                url = "https://www.hankyung.com" + url

            press    = self.safe_text(card, "span.author") or self.safe_text(card, "span.reporter") or "í•œêµ­ê²½ì œ"
            pub_time = (self.safe_text(card, "span.date")
                        or self.safe_text(card, "time")
                        or self.safe_attr(card, "time", "datetime"))

            items.append(NewsItem(title=title, press=press,
                                  pub_time=pub_time, url=url, source=self.SITE_KEY))
        return items


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  í†µí•© í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class MultiSiteCrawler:
    """
    3ëŒ€ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì¸í„°íŽ˜ì´ìŠ¤ë¡œ í†µí•© ì‹¤í–‰í•˜ëŠ” ë§¤ë‹ˆì €.

    ì‚¬ìš© ì˜ˆì‹œ:
        crawler = MultiSiteCrawler(sites=["naver", "daum", "hankyung"])
        results = crawler.crawl(keyword="ì‚¼ì„±ì „ìž", pages_per_site=3)
        df = crawler.to_dataframe(results)

    ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì¡°:
        _REGISTRY ë”•ì…”ë„ˆë¦¬ì—ë§Œ ì¶”ê°€í•˜ë©´ ìƒˆ ì‚¬ì´íŠ¸ë¥¼ ë°”ë¡œ ì§€ì›í•©ë‹ˆë‹¤.

    ì¤‘ë³µ ì œê±°:
        URL ê¸°ì¤€ â†’ URL ì—†ìœ¼ë©´ ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°í•©ë‹ˆë‹¤.
    """

    # âœï¸ ìƒˆ ì‚¬ì´íŠ¸ ì¶”ê°€ ì‹œ ì—¬ê¸°ì—ë§Œ ë“±ë¡
    _REGISTRY: dict[str, type] = {
        "naver":    NaverCrawler,
        "daum":     DaumCrawler,
        "hankyung": HankyungCrawler,
    }

    def __init__(self, sites=None, headless: bool = True, wait_sec: int = 8):
        target_keys = sites or list(self._REGISTRY.keys())
        invalid = set(target_keys) - set(self._REGISTRY)
        if invalid:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {invalid} | ì‚¬ìš© ê°€ëŠ¥: {list(self._REGISTRY)}")
        self.crawlers = [
            self._REGISTRY[k](headless=headless, wait_sec=wait_sec)
            for k in target_keys
        ]

    def crawl(self, keyword: str, pages_per_site: int = 3) -> list:
        """ë“±ë¡ëœ ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        all_items = []
        print(f"\n  ðŸŒ ë©€í‹°ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œìž‘")
        print(f"  í‚¤ì›Œë“œ: [{keyword}] | ì‚¬ì´íŠ¸ë‹¹ {pages_per_site}íŽ˜ì´ì§€")
        print(f"  ëŒ€ìƒ: {[c.SITE_NAME for c in self.crawlers]}")

        for crawler in self.crawlers:
            print(f"\n  â”€â”€â”€ {crawler.SITE_NAME} â”€â”€â”€")
            items = crawler.crawl(keyword=keyword, pages=pages_per_site)
            all_items.extend(items)
            print(f"  âœ… {crawler.SITE_NAME}: {len(items)}ê±´")

        all_items = self._deduplicate(all_items)
        print(f"\n  ðŸ“¦ ì´ ìˆ˜ì§‘: {len(all_items)}ê±´ (ì¤‘ë³µ ì œê±° í›„)")
        return all_items

    def to_dataframe(self, items: list):
        import pandas as pd
        return pd.DataFrame([i.to_dict() for i in items])

    def crawl_to_df(self, keyword: str, pages_per_site: int = 3):
        """crawl() + to_dataframe() íŽ¸ì˜ ë©”ì„œë“œ."""
        return self.to_dataframe(self.crawl(keyword=keyword, pages_per_site=pages_per_site))

    @staticmethod
    def _deduplicate(items: list) -> list:
        seen = set()
        unique = []
        for item in items:
            key = item.url.strip() if item.url.strip() else item.title.strip()
            if key not in seen:
                seen.add(key)
                unique.append(item)
        return unique

    @classmethod
    def list_sites(cls) -> list:
        return list(cls._REGISTRY.keys())
